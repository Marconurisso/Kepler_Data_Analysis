{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # math\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import plotly.express as px # ROC curve plot\n",
    "\n",
    "import seaborn as sns # fancy plotting\n",
    "import scipy # dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [sns.color_palette('rocket')[4],sns.color_palette('seismic')[0]] # color palette\n",
    "sns.set_theme(style = \"darkgrid\",palette= 'magma') # plot theme"
   ]
  },
  {
   "source": [
    "## Data Cleaning <a name=\"data_cleaning\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "# Read data\n",
    "kepler = pd.read_csv('C:/Users/marco/Desktop/Scuola/Data spaces/Tesina/Data/cumulative_2.csv')\n",
    "# Remove uninteresting variables\n",
    "data = kepler.drop(['kepid','kepoi_name','koi_vet_date','kepler_name','koi_vet_stat','koi_disposition','koi_score','koi_fpflag_nt','koi_fpflag_ss','koi_fpflag_co','koi_fpflag_ec','koi_disp_prov','koi_comment','koi_model_dof','koi_model_chisq','koi_limbdark_mod','koi_parm_prov','koi_tce_delivname','koi_bin_oedp_sig','koi_quarters','koi_trans_mod','koi_datalink_dvr','koi_datalink_dvs','koi_sparprov','koi_fittype','koi_max_sngle_ev','koi_max_mult_ev','koi_num_transits','koi_model_snr'],axis = 1)\n",
    "\n",
    "# Remove also the predictors from Pixel-Based KOI Vetting Statistics, Flux weighter analysis\n",
    "for col in data.columns:\n",
    "    if ('fwm' in col) or ('dicco' in col) or ('dikco' in col):\n",
    "        data = data.drop([col], axis = 1)\n",
    "\n",
    "# Categorize the response variable\n",
    "data['koi_pdisposition'] = data['koi_pdisposition'].astype('category')\n",
    "\n",
    "# Simplify the names\n",
    "data.columns = [col.replace('koi_','',1) for col in data.columns]\n",
    "data = data.rename({'pdisposition':'disposition'},axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to see the max, min and number of NaNs for each column\n",
    "def description(data):\n",
    "    mins = [data[col].min() for col in data.iloc[:,1:].columns]\n",
    "    maxs = [data[col].max() for col in data.iloc[:,1:].columns]\n",
    "    nans = [data[col].isnull().sum() for col in data.iloc[:,1:].columns]\n",
    "    means = [data[col].mean() for col in data.iloc[:,1:].columns]\n",
    "    maxmin = pd.DataFrame({'minimum': mins,'maximum': maxs, 'mean':means, 'NaNs': nans}, index = data.iloc[:,1:].columns)\n",
    "    return(maxmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the variables with only NaN\n",
    "data = data.drop(['longp','ingress','sage'], axis = 1)\n",
    "# Observe that eccen, ldm_coeff4 and ldm_coeff3 assume only value 0\n",
    "# They give us no information therefore we remove them\n",
    "data = data.drop(['eccen','ldm_coeff4','ldm_coeff3'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time0bk is time0 minus an offset constant therefore they can be described by just one variable\n",
    "sns.scatterplot(data = data, x='time0bk', y='time0',color = colors[1] ,s= 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decide to keep 'time0bk' as its values are smaller\n",
    "data = data.drop(['time0'], axis = 1)"
   ]
  },
  {
   "source": [
    "## FEATURE TRANSFORMATION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewed distributions of the predictors\n",
    "cols = data.iloc[:,1:].columns\n",
    "fig, axes = plt.subplots(1,3,figsize=(16,4))\n",
    "for i in [1,2,3]:\n",
    "    sns.histplot(data = data, x =cols[i], color = colors[1], ax = axes[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse hyperbolic sine function\n",
    "sns.lineplot(x = np.linspace(-14,14,200), y =np.arcsinh(np.linspace(-14,14,200)), color = colors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Iiverse hyperbolic sine function defined as\n",
    "# IHS(x) = log(x + sqrt(x^2+1)) to transform the pathological predictors\n",
    "data['period'] = np.arcsinh(data['period'])\n",
    "data['time0bk'] = np.arcsinh(data['time0bk'])\n",
    "data['impact'] = np.arcsinh(data['impact'])\n",
    "data['duration'] = np.arcsinh(data['duration'])\n",
    "data['depth'] = np.arcsinh(data['depth'])\n",
    "data['ror'] = np.arcsinh(data['ror'])\n",
    "data['srho'] = np.arcsinh(data['srho'])\n",
    "data['prad'] = np.arcsinh(data['prad'])\n",
    "data['insol'] = np.arcsinh(data['insol'])\n",
    "data['dor'] = np.arcsinh(data['dor'])\n",
    "data['srad'] = np.arcsinh(data['srad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize=(16,4))\n",
    "for i in [1,2,3]:\n",
    "    sns.histplot(data = data, x =cols[i], color = colors[1], ax = axes[i-1])"
   ]
  },
  {
   "source": [
    "## FEATURE SELECTION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for corrplot masking\n",
    "def mask_corr(corr):\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the absolute correlation values between predictors\n",
    "plt.figure(figsize = (14,14))\n",
    "sns.heatmap(abs(data.corr()), cmap = 'magma',center = 0.4, mask = mask_corr(data.corr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us look at the magnitude ('mag') variables \n",
    "sns.heatmap(abs(data.iloc[:,26:33].corr()), center = 0.4, cmap = 'rocket', mask = mask_corr(data.iloc[:,26:33].corr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = data.iloc[:,[25,26,27,28,29,30,31,32]], diag_kind=\"kde\", plot_kws={\"s\": 14,\"color\":colors[1]},diag_kws= {'color': colors[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two groups can be distinguished associated with high in-group correlation (https://en.wikipedia.org/wiki/Photometric_system)\n",
    "# - group1: 'kepmag', 'gmag', 'rmag', 'imag' -> visible and near infrared band\n",
    "# - group2: 'zmag', 'jmag', 'hmag', 'kmag' -> infrared band\n",
    "# we then keep only one for each group (the ones with the lowest amount of missing data): 'kepmag' and 'hmag'\n",
    "data = data.drop(['gmag','rmag','imag','zmag','jmag','kmag'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description(data)"
   ]
  },
  {
   "source": [
    "## MISSING DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now drop the missing data\n",
    "data = data.dropna()\n",
    "print(f\"The dataset had {raw_data.shape[0]} rows. It now has {data.shape[0]} rows.\\n({raw_data.shape[0]-data.shape[0]} rows were dropped, leaving us with {round(((data.shape[0]/raw_data.shape[0])*100),2)}% of the original number of entries.)\")"
   ]
  },
  {
   "source": [
    "## FEATURE HIERARCHICAL CLUSTERING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(data.iloc[:,1:].transpose(), 'single',metric='correlation')\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "dendrogram(linked,\n",
    "            orientation='right',\n",
    "            labels=data.iloc[:,1:].columns,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## OUTLIERS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are still outliers present\n",
    "sns.scatterplot(data = data,x='ror',y='srho',color = colors[1],s = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = data,x='sma',y='impact',color = colors[1],s = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(x, data):\n",
    "    x_mu = x - np.mean(data)\n",
    "    cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left = np.dot(x_mu, inv_covmat)\n",
    "    mahal = np.dot(left, x_mu.T)\n",
    "    return mahal.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah =mahalanobis(x = data.iloc[:,1:], data = data.iloc[:,1:])\n",
    "t = np.linspace(0,3.5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survival(t):\n",
    "    s = []\n",
    "    for i in t:\n",
    "        s.append(data[np.log10(mah)>i].shape[0]/data.shape[0])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log10 of the mahalanobis distance with the fraction of lost data by removing outliers at a certain threshold\n",
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "sns.boxenplot(np.log10(mahalanobis(x = data.iloc[:,1:], data = data.iloc[:,1:])),color = colors[0],saturation = 0.7)\n",
    "ax2 = ax.twinx()\n",
    "plt.plot(t,survival(t),color = colors[1])\n",
    "plt.grid(b=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By cutting outliers with distance greater than 2 we loose a fraction of the data equal to\n",
    "survival([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[np.log10(mahalanobis(x = data.iloc[:,1:], data = data.iloc[:,1:]))<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = data,x='sma',y='impact',color = colors[1],s = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = data,x='ror',y='srho',color = colors[1],s = 14)\n"
   ]
  },
  {
   "source": [
    "## EXPLORATORY DATA ANALYSIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes are balanced\n",
    "sns.histplot(data = data, x = 'disposition', hue = 'disposition', palette = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.iloc[:,1:].columns\n",
    "fig, axes = plt.subplots(2,5,figsize=(20,8))\n",
    "for i in range(5):\n",
    "    sns.kdeplot(data = data, x =cols[i],hue='disposition', palette = colors, ax = axes[(0,i)],fill=True, legend = False)\n",
    "for i in range(5):\n",
    "    sns.kdeplot(data = data, x =cols[i+5],hue='disposition', palette = colors, ax = axes[(1,i)],fill=True,legend = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,5,figsize=(20,8))\n",
    "for i in range(5):\n",
    "    sns.kdeplot(data = data, x =cols[i+10],hue='disposition', palette = colors, ax = axes[(0,i)],fill=True,legend = False)\n",
    "for i in range(5):\n",
    "    if i <= 1:\n",
    "        sns.histplot(data = data, x =cols[i+15],hue='disposition', palette = colors, ax = axes[(1,i)],legend = False, multiple = 'dodge')\n",
    "    else: \n",
    "        sns.kdeplot(data = data, x =cols[i+15],hue='disposition', palette = colors, ax = axes[(1,i)],fill=True,legend = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,5,figsize=(20,4))\n",
    "for i in range(5):\n",
    "    sns.kdeplot(data = data, x =cols[i+20],hue='disposition', palette = colors, ax = axes[i], fill =True,legend = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sky patch observed by Kepler\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.scatterplot(data = data, x= 'ra', y='dec', palette = colors, hue = 'disposition', s= 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export clean dataset\n",
    "data.to_csv('C:/Users/marco/Desktop/Scuola/Data spaces/Tesina/Data/cumulative_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc, f1_score, recall_score, precision_score, plot_confusion_matrix, plot_roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clean dataset\n",
    "data = pd.read_csv('C:/Users/marco/Desktop/Scuola/Data spaces/Tesina/Data/cumulative_clean.csv')\n",
    "data = data.iloc[:,1:]\n",
    "data['disposition'] = data['disposition'].astype('category')\n",
    "\n",
    "X = data.iloc[:,1:]\n",
    "Y = data.iloc[:,0]\n",
    "\n",
    "# 1 -> CANDIDATE, 0 -> FALSE POSITIVE\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['CANDIDATE','FALSE POSITIVE'])\n",
    "Y = le.transform(Y)\n",
    "Y = 1-Y\n",
    "\n",
    "# Train-Test data split\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.3, random_state = 0, stratify = Y )\n",
    "# Scaling for PCA\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for grouping grid search results\n",
    "def grid_search_groupby(results: pd.DataFrame, param_1: str, param_2: str) -> pd.DataFrame:\n",
    "\n",
    "    assert (type(results) ==  type(pd.DataFrame())), 'results should be a pandas.core.frame.DataFrame'\n",
    "    assert (type(param_1) == str), 'param_1 should be a string'\n",
    "    assert (type(param_2) == str), 'param_2 should be a string'\n",
    "    \n",
    "    params_df  = pd.DataFrame.from_dict(list(results.params.values))\n",
    "    mean_test_score = results.mean_test_score\n",
    "    result_shrt_df = pd.concat([mean_test_score, params_df], axis=1)\n",
    "    result_groupby = result_shrt_df.groupby([param_1, param_2])['mean_test_score'].mean().unstack()\n",
    "    return result_groupby\n",
    "\n",
    "# Function for plotting grid search results\n",
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2, s, log = False):\n",
    "    # Get Test Scores Mean for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']    \n",
    "    mean = np.zeros((len(grid_param_2),len(grid_param_1)))\n",
    "\n",
    "    for idx,val in enumerate(grid_param_2):\n",
    "        mean[idx,:] = [k for id,k in enumerate(scores_mean) if cv_results['params'][id][name_param_2]==val]\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _,ax = plt.subplots(1,1,figsize=(10,6))\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):    \n",
    "        ax.plot(grid_param_1, mean[idx,:], '-o', label= name_param_2 + ': ' + str(val), markersize=s)\n",
    "    if log == True:\n",
    "        ax.set_xscale('log')\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)"
   ]
  },
  {
   "source": [
    "## PCA "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance curve\n",
    "pca = PCA(n_components = 26)\n",
    "pca.fit(train_X_scaled)\n",
    "\n",
    "variance = pca.explained_variance_ratio_ \n",
    "var=np.cumsum(variance)\n",
    "plt.ylabel('Fraction of variance explained')\n",
    "plt.xlabel('# of principal components')\n",
    "plt.title('PCA Analysis')\n",
    "plt.plot(np.arange(1,27), var)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_pca = pca.transform(train_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the first two principal components \n",
    "plt.figure(figsize = (7,7))\n",
    "ax = sns.scatterplot(x= train_X_pca[:,0], y=train_X_pca[:,1], palette = colors, hue = 1- train_Y, legend = False, s= 14)\n",
    "ax.set(xlabel='PC 1', ylabel='PC 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the first three principal components \n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=train_X_pca[:,0],\n",
    "    y=train_X_pca[:,1],\n",
    "    z=train_X_pca[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=[tuple(256*x for x in colors[0]) if p == 1 else tuple(256*x for x in colors[1])  for p in train_Y],              \n",
    "        opacity=0.7\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1600,\n",
    "    height=600,\n",
    "    scene = dict(\n",
    "    xaxis_title=\"PC 1\",\n",
    "    yaxis_title=\"PC 2\",\n",
    "    zaxis_title=\"PC 3\")\n",
    "    )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "source": [
    "## KNN "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can start with the simplest model knn and perform grid search cross validation to tune the hyperparameters:\n",
    "# -the number of neighbors considered;\n",
    "# -the norm used {Euclidean, Manhattan, Chebyshev}.\n",
    "grid = {'n_neighbors': range(1,90),'metric':['euclidean','manhattan','chebyshev']}\n",
    "knn =  neighbors.KNeighborsClassifier()\n",
    "pipe = Pipeline(steps=[('scaler', StandardScaler()), ('knn', knn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search without scaling\n",
    "knn_grid = GridSearchCV(estimator = knn, param_grid = grid, cv = 4, verbose=2, n_jobs = -1)\n",
    "knn_grid.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search result without scaling\n",
    "plot_grid_search(knn_grid.cv_results_, grid['n_neighbors'], grid['metric'], 'n_neighbors', 'metric', s = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'knn__n_neighbors': range(1,90),'knn__metric':['euclidean','manhattan','chebyshev']}\n",
    "knn_grid_scaled = GridSearchCV(estimator = pipe, param_grid = grid, cv = 4, verbose=2, n_jobs = -1)\n",
    "knn_grid_scaled.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search result with scaling\n",
    "plot_grid_search(knn_grid_scaled.cv_results_, grid['knn__n_neighbors'], grid['knn__metric'], 'knn__n_neighbors', 'knn__metric', s = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scaled one performs much better\n",
    "best_knn = Pipeline(steps=[('scaler', StandardScaler()), ('knn', neighbors.KNeighborsClassifier(n_neighbors=20, metric = 'manhattan'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validated score is\n",
    "cross_val_score(best_knn,train_X,train_Y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'ccp_alpha':np.arange(0,0.03,0.001), 'criterion':['gini','entropy']}\n",
    "tree = DecisionTreeClassifier(random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid = GridSearchCV(estimator = tree, param_grid = grid, cv = 10, verbose=2, n_jobs = -1)\n",
    "tree_grid.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search results\n",
    "plot_grid_search(tree_grid.cv_results_, np.arange(0,0.03,0.001), ['gini','entropy'], 'ccp_alpha', 'criterion',s = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more interpretable tree can be optained by using a stronger pruning parameter without loosing too much accuracy\n",
    "interp_tree = DecisionTreeClassifier(criterion = 'entropy', ccp_alpha = 0.005, random_state = 1)\n",
    "interp_tree.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tree\n",
    "plt.figure(num=None, figsize=(20, 6), dpi=80)\n",
    "plot_tree(interp_tree, filled = True, proportion = True,  feature_names=X.columns  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validated score is\n",
    "cross_val_score(interp_tree, train_X, train_Y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = {'n_estimators': range(1,300,10), 'criterion':['entropy']}, cv = 4, verbose=2, n_jobs = -1 )\n",
    "rf_grid.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search results\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plot_grid_search(rf_grid.cv_results_, range(1,300,10), ['entropy'], 'n_estimators', 'criterion',s = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 trees seems to be enough for reaching convergence\n",
    "rf2 = RandomForestClassifier(n_estimators=100, criterion = 'entropy')\n",
    "rf2_grid = GridSearchCV(estimator = rf2, param_grid = {'max_depth':np.arange(2,20,2),'min_samples_split':np.arange(2,40,2)}, cv = 4, verbose=2, n_jobs = -1 )\n",
    "rf2_grid.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf3 = RandomForestClassifier(n_estimators=100, criterion = 'gini')\n",
    "rf3_grid = GridSearchCV(estimator = rf2, param_grid = {'max_depth':np.arange(2,20,2),'min_samples_split':np.arange(2,40,2)}, cv = 4, verbose=2, n_jobs = -1 )\n",
    "rf3_grid.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_entropy = pd.DataFrame(rf2_grid.cv_results_)\n",
    "results_gini = pd.DataFrame(rf3_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped_entropy = grid_search_groupby(results_entropy,'max_depth','min_samples_split')\n",
    "results_grouped_gini = grid_search_groupby(results_gini,'max_depth','min_samples_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize = (12,6))\n",
    "sns.heatmap(results_grouped_entropy, cmap = 'magma', center = 0.83,ax = ax1)\n",
    "ax1.set_title('Entropy')\n",
    "ax2.set_title('Gini')\n",
    "sns.heatmap(results_grouped_gini, cmap = 'magma', center = 0.83, ax = ax2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best CV score Entropy\n",
    "[rf2_grid.best_params_, rf2_grid.best_score_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best CV score Gini\n",
    "[rf3_grid.best_params_, rf3_grid.best_score_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = RandomForestClassifier(n_estimators = 100, max_depth = 18, min_samples_split = 6, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validated score is\n",
    "cross_val_score(best_rf,train_X,train_Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of feature importance\n",
    "importance = pd.DataFrame({'name':X.columns,'importance':best_rf.feature_importances_})\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data = importance,x='name',y='importance', palette = 'magma')\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "source": [
    "## SUPPORT VECTOR MACHINE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "source": [
    "### Linear"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'linear__C' : [0.001,0.01,0.1,1,10,100,1000,10000], 'linear__penalty':['l1','l2']}\n",
    "scaler = StandardScaler()\n",
    "linear = LinearSVC(max_iter=10000)\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('linear', linear)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_grid = GridSearchCV(estimator = pipe, param_grid = grid, cv = 4, verbose=2, n_jobs = -1 )\n",
    "linear_grid.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search results\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plot_grid_search(linear_grid.cv_results_, grid['linear__C'], grid['linear__penalty'], 'linear__C', 'linear__penalty',s = 5, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_linear = Pipeline(steps=[('scaler', StandardScaler()), ('linear', LinearSVC(max_iter = 10000, penalty = 'l2', C = 1e-2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validated score is\n",
    "cross_val_score(best_linear, train_X, train_Y).mean()"
   ]
  },
  {
   "source": [
    "### Gaussian RBF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'rbf__gamma': [1e-10,1e-9,1e-8,1e-7,1e-6,1e-5,1e-4, 1e-3,1e-2,0.1,10,100],'rbf__C' : [0.0001,0.001,0.01,0.1,1,10,100,1e3,1e4,1e5]}\n",
    "scaler = StandardScaler()\n",
    "rbf = SVC(kernel = 'rbf')\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('rbf', rbf )])\n",
    "rbf_grid = GridSearchCV(estimator = pipe, param_grid = grid, cv = 4, verbose=10, n_jobs = -1 )\n",
    "rbf_grid.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(rbf_grid.cv_results_)\n",
    "results_grouped= grid_search_groupby(results,'rbf__gamma','rbf__C')\n",
    "sns.heatmap(results_grouped, cmap = 'magma', center = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rbf = Pipeline(steps=[('scaler', StandardScaler()), ('rbf', SVC(kernel = 'rbf', C = 1, gamma = 0.1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validated score is\n",
    "cross_val_score(best_rbf, train_X, train_Y).mean()"
   ]
  },
  {
   "source": [
    "# CONCLUSIONS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Results on the test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best estimators\n",
    "best_rbf.fit(train_X,train_Y)\n",
    "best_linear.fit(train_X,train_Y)\n",
    "best_knn.fit(train_X,train_Y)\n",
    "interp_tree.fit(train_X,train_Y)\n",
    "best_rf.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVES\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "plot_roc_curve(best_knn, test_X, test_Y, ax = ax, name = 'KNN')\n",
    "plot_roc_curve(interp_tree, test_X, test_Y, ax = ax, name = 'Tree classifier')\n",
    "plot_roc_curve(best_rf, test_X, test_Y, ax = ax, name = 'Random Forest')\n",
    "plot_roc_curve(best_rbf, test_X, test_Y, ax = ax, name = 'RBF SVM')\n",
    "plot_roc_curve(best_linear, test_X, test_Y, ax = ax, name = 'Linear SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "ax.set_xlim(0.02, 0.35)\n",
    "ax.set_ylim(0.4, 1)\n",
    "plot_roc_curve(best_knn, test_X, test_Y, ax = ax, name = 'KNN')\n",
    "plot_roc_curve(interp_tree, test_X, test_Y, ax = ax, name = 'Tree classifier')\n",
    "plot_roc_curve(best_rf, test_X, test_Y, ax = ax, name = 'Random Forest')\n",
    "plot_roc_curve(best_rbf, test_X, test_Y, ax = ax, name = 'RBF SVM')\n",
    "plot_roc_curve(best_linear, test_X, test_Y, ax = ax, name = 'Linear SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols = 3,figsize=(14,4))\n",
    "plot_confusion_matrix(best_knn, test_X, test_Y, ax = ax1, cmap = 'magma', colorbar = False)\n",
    "plot_confusion_matrix(interp_tree, test_X, test_Y, ax = ax2, cmap = 'magma', colorbar = False)\n",
    "plot_confusion_matrix(best_rf, test_X, test_Y, ax = ax3, cmap = 'magma', colorbar = False)\n",
    "ax1.set_title('KNN')\n",
    "ax2.set_title('Tree')\n",
    "ax3.set_title('Forest')\n",
    "ax1.grid(b=None)\n",
    "ax2.grid(b=None)\n",
    "ax3.grid(b=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, (ax1,ax2) = plt.subplots(ncols = 2,figsize=(9,4))\n",
    "plot_confusion_matrix(best_linear, test_X, test_Y, ax = ax1, cmap = 'magma', colorbar = False)\n",
    "plot_confusion_matrix(best_rbf, test_X, test_Y, ax = ax2, cmap = 'magma', colorbar = False)\n",
    "ax1.set_title('Linear SVM')\n",
    "ax2.set_title('RBF SVM')\n",
    "ax1.grid(b=None)\n",
    "ax2.grid(b=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validated scores\n",
    "df = pd.DataFrame({'Name':[],'Score':[],'Type':[]})\n",
    "estimators = [best_knn,interp_tree, best_rf,best_rbf,best_linear]\n",
    "names = ['KNN','Tree','Random Forest','RBF SVM','Linear SVM']\n",
    "for i in range(len(estimators)):\n",
    "    df.loc[2*i] = [names[i], cross_val_score(estimators[i],train_X,train_Y, scoring = 'precision').mean(),'Precision']\n",
    "    df.loc[2*i+1] = [names[i], cross_val_score(estimators[i],train_X,train_Y, scoring = 'recall').mean(),'Recall']\n",
    "sns.catplot(data = df, kind  = 'bar', x ='Name', y = 'Score', hue = 'Type', alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set scores\n",
    "df_test = pd.DataFrame({'Name':[],'Score':[],'Type':[]})\n",
    "for i in range(len(estimators)):\n",
    "    df_test.loc[2*i] = [names[i], cross_val_score(estimators[i],train_X,train_Y, scoring = 'precision').mean(),'Precision']\n",
    "    df_test.loc[2*i+1] = [names[i], cross_val_score(estimators[i],train_X,train_Y, scoring = 'recall').mean(),'Recall']\n",
    "sns.catplot(data = df_test, kind  = 'bar', x ='Name', y = 'Score', hue = 'Type', alpha = 0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd022d38fa0cb949c4d83b127034afdff90d77a7338f5681221558c482c7c131893",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}